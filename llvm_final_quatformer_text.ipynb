{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV-noix4Qu0C",
        "outputId": "bdb48e04-25e6-41af-818e-13a15b006f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import logging as transformers_logging\n",
        "transformers_logging.set_verbosity_error()  # Suppress warnings from transformers\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from torch.nn import Parameter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install -U datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "F2qYUCEqXWKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "### Converts raw text data from HuggingFace into a cleaned format suitable for BERT and other NLP models"
      ],
      "metadata": {
        "id": "IHgcPo09e5tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocessor:\n",
        "    # INITIALIZE WITH BERT TOKENIZER\n",
        "    def __init__(self, pretrained_model_name='bert-base-uncased', max_len=50):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
        "        self.max_len = max_len\n",
        "        # Create translation table once during initialization\n",
        "        self.punctuation_translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    # TEXT CLEANING - REMOVE HTML TAGS AND PUNCTUATION, CONVERT TO LOWERCASE\n",
        "    def clean_text(self, text):\n",
        "        # Skip BeautifulSoup for simple cases\n",
        "        text = re.sub(r'<[^>]+>', '', text)  # Faster than BeautifulSoup for basic HTML\n",
        "        return text.lower().translate(self.punctuation_translator)\n",
        "\n",
        "    # Vectorized version of clean_text for processing entire series at once\n",
        "    def clean_text_batch(self, text_series):\n",
        "        # Process the entire series at once using vectorized operations\n",
        "        text_series = text_series.str.replace(r'<[^>]+>', '', regex=True)\n",
        "        text_series = text_series.str.lower()\n",
        "        text_series = text_series.apply(lambda x: x.translate(self.punctuation_translator))\n",
        "        return text_series\n",
        "\n",
        "    # ENCODE TEXT WITH BERT TOKENIZER\n",
        "    def tokenize_and_encode(self, texts):\n",
        "        return self.tokenizer(\n",
        "            texts,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "    # PREPROCESS USING ALL ABOVE METHODS\n",
        "    def preprocess_dataset(self, df_or_dfs, text_column, label_column):\n",
        "        if not isinstance(df_or_dfs, list) or len(df_or_dfs) != 3:\n",
        "            raise ValueError(\"df_or_dfs must be a DataFrame or a list of DataFrames of size 3.\")\n",
        "\n",
        "        train_df, val_df, test_df = df_or_dfs\n",
        "\n",
        "        print(\"Cleaning text data...\")\n",
        "        # Use vectorized operations instead of apply\n",
        "        train_df[text_column] = self.clean_text_batch(train_df[text_column])\n",
        "        val_df[text_column] = self.clean_text_batch(val_df[text_column])\n",
        "        test_df[text_column] = self.clean_text_batch(test_df[text_column])\n",
        "\n",
        "        print(\"Tokenizing and encoding text data...\")\n",
        "        # Convert to lists only once before tokenization\n",
        "        train_texts = train_df[text_column].tolist()\n",
        "        val_texts = val_df[text_column].tolist()\n",
        "        test_texts = test_df[text_column].tolist()\n",
        "\n",
        "        train_encodings = self.tokenize_and_encode(train_texts)\n",
        "        val_encodings = self.tokenize_and_encode(val_texts)\n",
        "        test_encodings = self.tokenize_and_encode(test_texts)\n",
        "\n",
        "        # Convert labels to tensors directly from original dataframes\n",
        "        train_labels = torch.tensor(train_df[label_column].values)\n",
        "        val_labels = torch.tensor(val_df[label_column].values)\n",
        "        test_labels = torch.tensor(test_df[label_column].values)\n",
        "\n",
        "        print(f\"Train set: {len(train_df)} samples\")\n",
        "        print(f\"Validation set: {len(val_df)} samples\")\n",
        "        print(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "        return (train_encodings['input_ids'], train_encodings['attention_mask'], train_labels,\n",
        "                val_encodings['input_ids'], val_encodings['attention_mask'], val_labels,\n",
        "                test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)"
      ],
      "metadata": {
        "id": "q-hMl5nDbUrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quaternion Classes"
      ],
      "metadata": {
        "id": "cYXKcb3qfhmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Quaternion:\n",
        "    # INITIALIZE TENSOR THAT WILL BE SPLIT INTO QUATERNION COMPONENTS\n",
        "    def __init__(self, tensor):\n",
        "        # ASSUME COMPONENTS CONCATENATED ALONG LAST DIMENSION\n",
        "        self.tensor = tensor\n",
        "        # Store the shape for faster processing\n",
        "        self.quat_dim = self.tensor.shape[-1] // 4\n",
        "        # Split only when needed (lazy evaluation)\n",
        "        self._components = None\n",
        "\n",
        "    @property\n",
        "    def components(self):\n",
        "        # Lazy initialization of components\n",
        "        if self._components is None:\n",
        "            self._components = torch.split(self.tensor, self.quat_dim, dim=-1)\n",
        "        return self._components\n",
        "\n",
        "    @property\n",
        "    def r(self):\n",
        "        return self.components[0]\n",
        "\n",
        "    @property\n",
        "    def i(self):\n",
        "        return self.components[1]\n",
        "\n",
        "    @property\n",
        "    def j(self):\n",
        "        return self.components[2]\n",
        "\n",
        "    @property\n",
        "    def k(self):\n",
        "        return self.components[3]\n",
        "\n",
        "    @classmethod\n",
        "    # CREATE QUATERNION FROM INPUT R, I, J, K COMPONENTS\n",
        "    def from_components(cls, r, i, j, k):\n",
        "        tensor = torch.cat([r, i, j, k], dim=-1)\n",
        "        return cls(tensor)\n",
        "\n",
        "    # COMPUTE HAMILTON PRODUCT BETWEEN TWO QUATERNIONS\n",
        "    def hamilton_product(self, other):\n",
        "        r1, i1, j1, k1 = self.r, self.i, self.j, self.k\n",
        "        r2, i2, j2, k2 = other.r, other.i, other.j, other.k\n",
        "\n",
        "        # Simplified condition check\n",
        "        is_matrix_mul = (r1.dim() == 2 and r2.dim() == 2 and r1.shape[-1] == r2.shape[-2])\n",
        "\n",
        "        if is_matrix_mul:\n",
        "            # Matrix multiplication - pre-allocate output tensors to optimize memory\n",
        "            batch_size = r1.size(0)\n",
        "            output_size = r2.size(-1)\n",
        "\n",
        "            r = torch.zeros(batch_size, output_size, device=r1.device)\n",
        "            i = torch.zeros(batch_size, output_size, device=r1.device)\n",
        "            j = torch.zeros(batch_size, output_size, device=r1.device)\n",
        "            k = torch.zeros(batch_size, output_size, device=r1.device)\n",
        "\n",
        "            # Use in-place operations where possible\n",
        "            torch.matmul(r1, r2, out=r)\n",
        "            r.sub_(torch.matmul(i1, i2))\n",
        "            r.sub_(torch.matmul(j1, j2))\n",
        "            r.sub_(torch.matmul(k1, k2))\n",
        "\n",
        "            torch.matmul(r1, i2, out=i)\n",
        "            i.add_(torch.matmul(i1, r2))\n",
        "            i.add_(torch.matmul(j1, k2))\n",
        "            i.sub_(torch.matmul(k1, j2))\n",
        "\n",
        "            torch.matmul(r1, j2, out=j)\n",
        "            j.sub_(torch.matmul(i1, k2))\n",
        "            j.add_(torch.matmul(j1, r2))\n",
        "            j.add_(torch.matmul(k1, i2))\n",
        "\n",
        "            torch.matmul(r1, k2, out=k)\n",
        "            k.add_(torch.matmul(i1, j2))\n",
        "            k.sub_(torch.matmul(j1, i2))\n",
        "            k.add_(torch.matmul(k1, r2))\n",
        "        else:\n",
        "            # Element-wise multiplication - use fused operations\n",
        "            r = r1 * r2 - i1 * i2 - j1 * j2 - k1 * k2\n",
        "            i = r1 * i2 + i1 * r2 + j1 * k2 - k1 * j2\n",
        "            j = r1 * j2 - i1 * k2 + j1 * r2 + k1 * i2\n",
        "            k = r1 * k2 + i1 * j2 - j1 * i2 + k1 * r2\n",
        "\n",
        "        # Pre-allocate tensor for output\n",
        "        output_tensor = torch.cat([r, i, j, k], dim=-1)\n",
        "        return Quaternion(output_tensor)\n",
        "\n",
        "    # COMPONENT-WISE ADDITION BETWEEN QUATERNIONS\n",
        "    def add(self, other):\n",
        "        # Directly add the tensors without component splitting\n",
        "        return Quaternion(self.tensor + other.tensor)\n",
        "\n",
        "    # CONVERT TO UNIT QUATERNION\n",
        "    def normalize(self):\n",
        "        # Calculate norm across all components at once\n",
        "        norm = torch.sqrt(torch.sum(self.tensor ** 2, dim=-1, keepdim=True) / 4)\n",
        "        return Quaternion(self.tensor / norm)\n",
        "\n",
        "    # CONJUGATE FORM - JUST NEGATE IMAGINARY PARTS\n",
        "    def conjugate(self):\n",
        "        # Create conjugate mask once [1, -1, -1, -1] repeated for each component\n",
        "        conj_mask = torch.ones_like(self.tensor)\n",
        "        conj_mask[..., self.quat_dim:] = -1\n",
        "        return Quaternion(self.tensor * conj_mask)\n",
        "\n",
        "    # CONVERT TO SINGLE CONCATENATED TENSOR\n",
        "    def as_tensor(self):\n",
        "        # Already have tensor, just ensure correct dimensions\n",
        "        tensor = self.tensor\n",
        "\n",
        "        # IF 2D TENSOR, UNSQUEEZE TO ADD A BATCH DIMENSION OF 1\n",
        "        if tensor.ndim == 2:\n",
        "            tensor = tensor.unsqueeze(0)\n",
        "        return tensor"
      ],
      "metadata": {
        "id": "suIotr12cIwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuaternionTransformation(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, activation=None, init=None):\n",
        "        super(QuaternionTransformation, self).__init__()\n",
        "\n",
        "        # Quaternion input dimension is divided by 4 (for r, i, j, k components)\n",
        "        self.input_dim = input_dim // 4\n",
        "        self.output_dim = output_dim // 4\n",
        "        self.activation = activation\n",
        "\n",
        "        # Create a single weight matrix for all components\n",
        "        # This allows for more efficient operations in some backends\n",
        "        self.all_weights = Parameter(torch.Tensor(4, self.input_dim, self.output_dim))\n",
        "\n",
        "        # Initialize the weights\n",
        "        if init is None:\n",
        "            nn.init.xavier_uniform_(self.all_weights)\n",
        "        else:\n",
        "            # Apply custom initialization if provided\n",
        "            for i in range(4):\n",
        "                self.all_weights.data[i] = init(self.all_weights.data[i])\n",
        "\n",
        "    @property\n",
        "    def r_weight(self):\n",
        "        return self.all_weights[0]\n",
        "\n",
        "    @property\n",
        "    def i_weight(self):\n",
        "        return self.all_weights[1]\n",
        "\n",
        "    @property\n",
        "    def j_weight(self):\n",
        "        return self.all_weights[2]\n",
        "\n",
        "    @property\n",
        "    def k_weight(self):\n",
        "        return self.all_weights[3]\n",
        "\n",
        "    # PERFORM QUATERNION TRANSFORMATION ON INPUT TENSOR\n",
        "    def forward(self, x):\n",
        "        # INPUT TENSOR SHAPE:\n",
        "        #   FOR 2D - [batch_size, input_dim]\n",
        "        #   FOR 3D - [batch_size, seq_len, input_dim]\n",
        "        # OUTPUT TENSOR SHAPE:\n",
        "        #   FOR 2D - [batch_size, output_dim*4]\n",
        "        #   FOR 3D - [batch_size, seq_len, output_dim*4]\n",
        "\n",
        "        # Store original shape for reshaping at the end\n",
        "        original_shape = x.shape\n",
        "        is_3d_input = x.ndim == 3\n",
        "\n",
        "        if is_3d_input:\n",
        "            # Reshape once for all operations\n",
        "            batch_size, seq_len, input_dim = original_shape\n",
        "            x = x.reshape(-1, input_dim)\n",
        "\n",
        "        # Split input into quaternion components (r, i, j, k)\n",
        "        quat_dim = x.shape[-1] // 4\n",
        "        r_x, i_x, j_x, k_x = torch.split(x, quat_dim, dim=-1)\n",
        "\n",
        "        # Use weight components directly instead of creating a quaternion object\n",
        "        r_w, i_w, j_w, k_w = self.r_weight, self.i_weight, self.j_weight, self.k_weight\n",
        "\n",
        "        # Perform Hamilton product more efficiently\n",
        "        # Pre-allocate tensors for output\n",
        "        r_out = torch.matmul(r_x, r_w) - torch.matmul(i_x, i_w) - torch.matmul(j_x, j_w) - torch.matmul(k_x, k_w)\n",
        "        i_out = torch.matmul(r_x, i_w) + torch.matmul(i_x, r_w) + torch.matmul(j_x, k_w) - torch.matmul(k_x, j_w)\n",
        "        j_out = torch.matmul(r_x, j_w) - torch.matmul(i_x, k_w) + torch.matmul(j_x, r_w) + torch.matmul(k_x, i_w)\n",
        "        k_out = torch.matmul(r_x, k_w) + torch.matmul(i_x, j_w) - torch.matmul(j_x, i_w) + torch.matmul(k_x, r_w)\n",
        "\n",
        "        # Concatenate output components\n",
        "        output = torch.cat([r_out, i_out, j_out, k_out], dim=-1)\n",
        "\n",
        "        # Apply activation function if provided\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "\n",
        "        if is_3d_input:\n",
        "            # Reshape back to original 3D shape\n",
        "            output = output.view(batch_size, seq_len, self.output_dim * 4)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "d1o36VTfi-LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuaternionSelfAttention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.0):\n",
        "        super(QuaternionSelfAttention, self).__init__()\n",
        "\n",
        "        # Ensure output_dim is divisible by 4 for quaternion representation\n",
        "        assert output_dim % 4 == 0, \"output_dim must be divisible by 4 for quaternion representation.\"\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.dk = self.output_dim // 4\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Use our optimized QuaternionTransformation\n",
        "        self.q_transform = QuaternionTransformation(self.output_dim, self.output_dim)\n",
        "        self.k_transform = QuaternionTransformation(self.output_dim, self.output_dim)\n",
        "        self.v_transform = QuaternionTransformation(self.output_dim, self.output_dim)\n",
        "\n",
        "        # Pre-compute the scaling factor\n",
        "        self.scale_factor = torch.sqrt(torch.tensor(self.dk, dtype=torch.float32))\n",
        "\n",
        "    # PERFORM DOT PRODUCT ATTENTION BETWEEN TWO QUATERNION SEQUENCES - OPTIMIZED\n",
        "    def quaternion_attention(self, a, b):\n",
        "        # Ensure the input dimensions are divisible by 4\n",
        "        assert a.size(-1) % 4 == 0, \"Last dimension of input tensor must be divisible by 4 for quaternion representation.\"\n",
        "        assert b.size(-1) % 4 == 0, \"Last dimension of input tensor must be divisible by 4 for quaternion representation.\"\n",
        "\n",
        "        # Split inputs into quaternion components - more efficiently\n",
        "        quat_dim = a.size(-1) // 4\n",
        "        ar, ai, aj, ak = torch.split(a, quat_dim, dim=-1)\n",
        "        br, bi, bj, bk = torch.split(b, quat_dim, dim=-1)\n",
        "\n",
        "        # Compute the quaternion Hamilton product - pre-allocate output tensors\n",
        "        r = (torch.matmul(ar, br.transpose(-1, -2)) -\n",
        "             torch.matmul(ai, bi.transpose(-1, -2)) -\n",
        "             torch.matmul(aj, bj.transpose(-1, -2)) -\n",
        "             torch.matmul(ak, bk.transpose(-1, -2)))\n",
        "\n",
        "        i = (torch.matmul(ar, bi.transpose(-1, -2)) +\n",
        "             torch.matmul(ai, br.transpose(-1, -2)) +\n",
        "             torch.matmul(aj, bk.transpose(-1, -2)) -\n",
        "             torch.matmul(ak, bj.transpose(-1, -2)))\n",
        "\n",
        "        j = (torch.matmul(ar, bj.transpose(-1, -2)) -\n",
        "             torch.matmul(ai, bk.transpose(-1, -2)) +\n",
        "             torch.matmul(aj, br.transpose(-1, -2)) +\n",
        "             torch.matmul(ak, bi.transpose(-1, -2)))\n",
        "\n",
        "        k = (torch.matmul(ar, bk.transpose(-1, -2)) +\n",
        "             torch.matmul(ai, bj.transpose(-1, -2)) -\n",
        "             torch.matmul(aj, bi.transpose(-1, -2)) +\n",
        "             torch.matmul(ak, br.transpose(-1, -2)))\n",
        "\n",
        "        # Create quaternion object directly with components\n",
        "        return Quaternion.from_components(r, i, j, k)\n",
        "\n",
        "    # FORWARD PASS - OPTIMIZED\n",
        "    def forward(self, X):\n",
        "        # Store original shape\n",
        "        original_shape = X.shape\n",
        "        is_3d_input = X.ndim == 3\n",
        "\n",
        "        # Compute quaternion transformations for Q, K, V\n",
        "        Q = self.q_transform(X)  # Shape: [batch_size, seq_len, output_dim]\n",
        "        K = self.k_transform(X)  # Shape: [batch_size, seq_len, output_dim]\n",
        "        V = self.v_transform(X)  # Shape: [batch_size, seq_len, output_dim]\n",
        "\n",
        "        # Split V into quaternion components - only once\n",
        "        V_comps = torch.split(V, self.dk, dim=-1)\n",
        "        V_r, V_i, V_j, V_k = V_comps\n",
        "\n",
        "        # Compute quaternion attention weights\n",
        "        attention_weights = self.quaternion_attention(Q, K)\n",
        "\n",
        "        # Pre-compute scaling factor once - already stored as instance variable\n",
        "        scaled_sqrt_dk = self.scale_factor\n",
        "\n",
        "        # Apply component-wise softmax normalization with vectorized scaling\n",
        "        attention_weights_r = F.softmax(attention_weights.r / scaled_sqrt_dk, dim=-1)\n",
        "        attention_weights_i = F.softmax(attention_weights.i / scaled_sqrt_dk, dim=-1)\n",
        "        attention_weights_j = F.softmax(attention_weights.j / scaled_sqrt_dk, dim=-1)\n",
        "        attention_weights_k = F.softmax(attention_weights.k / scaled_sqrt_dk, dim=-1)\n",
        "\n",
        "        # Apply dropout to the attention weights all at once using functional API\n",
        "        if self.dropout_rate > 0 and self.training:\n",
        "            attention_weights_r = F.dropout(attention_weights_r, p=self.dropout_rate, training=self.training)\n",
        "            attention_weights_i = F.dropout(attention_weights_i, p=self.dropout_rate, training=self.training)\n",
        "            attention_weights_j = F.dropout(attention_weights_j, p=self.dropout_rate, training=self.training)\n",
        "            attention_weights_k = F.dropout(attention_weights_k, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # Apply the attention weights to the V components - using batch matrix multiplication\n",
        "        attention_r = torch.bmm(attention_weights_r, V_r) if is_3d_input else attention_weights_r @ V_r\n",
        "        attention_i = torch.bmm(attention_weights_i, V_i) if is_3d_input else attention_weights_i @ V_i\n",
        "        attention_j = torch.bmm(attention_weights_j, V_j) if is_3d_input else attention_weights_j @ V_j\n",
        "        attention_k = torch.bmm(attention_weights_k, V_k) if is_3d_input else attention_weights_k @ V_k\n",
        "\n",
        "        # Concatenate the attended quaternion components\n",
        "        attention_output = torch.cat([attention_r, attention_i, attention_j, attention_k], dim=-1)\n",
        "\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "_l7VQVfTjFYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadQuaternionSelfAttention(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_heads=4, dropout_rate=0.0):\n",
        "        super(MultiHeadQuaternionSelfAttention, self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # Ensure output_dim is divisible by 4 * num_heads for quaternion-based operations\n",
        "        assert self.output_dim % (4 * num_heads) == 0, \"output_dim must be divisible by 4 * num_heads.\"\n",
        "\n",
        "        # Compute the output dimension for each head\n",
        "        self.output_dim_per_head = self.output_dim // num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Instead of creating many separate attention modules, create shared weight matrices\n",
        "        # with appropriate dimensionality, which can be more efficiently parallelized\n",
        "        self.q_projection = QuaternionTransformation(input_dim, self.output_dim)\n",
        "        self.k_projection = QuaternionTransformation(input_dim, self.output_dim)\n",
        "        self.v_projection = QuaternionTransformation(input_dim, self.output_dim)\n",
        "\n",
        "        # Final output projection\n",
        "        self.output_transform = QuaternionTransformation(self.output_dim, output_dim)\n",
        "\n",
        "        # Pre-compute the scale factor\n",
        "        self.scale_factor = torch.sqrt(torch.tensor(self.output_dim_per_head // 4, dtype=torch.float32))\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape input for multi-head attention\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Reshape: [batch_size, seq_len, num_heads, output_dim_per_head]\n",
        "        x = x.view(batch_size, seq_len, self.num_heads, self.output_dim_per_head)\n",
        "\n",
        "        # Transpose: [batch_size, num_heads, seq_len, output_dim_per_head]\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Combine multi-head attention results\n",
        "        batch_size, _, seq_len, _ = x.shape\n",
        "\n",
        "        # Transpose: [batch_size, seq_len, num_heads, output_dim_per_head]\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Reshape: [batch_size, seq_len, output_dim]\n",
        "        return x.reshape(batch_size, seq_len, self.output_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Get input tensor dimensions and store for reshaping\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "\n",
        "        # Apply projections to create Q, K, V\n",
        "        Q = self.q_projection(X)  # [batch_size, seq_len, output_dim]\n",
        "        K = self.k_projection(X)  # [batch_size, seq_len, output_dim]\n",
        "        V = self.v_projection(X)  # [batch_size, seq_len, output_dim]\n",
        "\n",
        "        # Split heads for Q, K, V\n",
        "        Q = self.split_heads(Q)  # [batch_size, num_heads, seq_len, output_dim_per_head]\n",
        "        K = self.split_heads(K)  # [batch_size, num_heads, seq_len, output_dim_per_head]\n",
        "        V = self.split_heads(V)  # [batch_size, num_heads, seq_len, output_dim_per_head]\n",
        "\n",
        "        # Split into quaternion components\n",
        "        head_dim = self.output_dim_per_head // 4\n",
        "\n",
        "        # Split Q, K components\n",
        "        Q_comps = torch.split(Q, head_dim, dim=-1)\n",
        "        Q_r, Q_i, Q_j, Q_k = Q_comps\n",
        "\n",
        "        K_comps = torch.split(K, head_dim, dim=-1)\n",
        "        K_r, K_i, K_j, K_k = K_comps\n",
        "\n",
        "        V_comps = torch.split(V, head_dim, dim=-1)\n",
        "        V_r, V_i, V_j, V_k = V_comps\n",
        "\n",
        "        # Compute attention scores using quaternion Hamilton product\n",
        "        # Pre-allocate tensors for matrix multiplications\n",
        "        scores_r = (torch.matmul(Q_r, K_r.transpose(-1, -2)) -\n",
        "                    torch.matmul(Q_i, K_i.transpose(-1, -2)) -\n",
        "                    torch.matmul(Q_j, K_j.transpose(-1, -2)) -\n",
        "                    torch.matmul(Q_k, K_k.transpose(-1, -2)))\n",
        "\n",
        "        scores_i = (torch.matmul(Q_r, K_i.transpose(-1, -2)) +\n",
        "                    torch.matmul(Q_i, K_r.transpose(-1, -2)) +\n",
        "                    torch.matmul(Q_j, K_k.transpose(-1, -2)) -\n",
        "                    torch.matmul(Q_k, K_j.transpose(-1, -2)))\n",
        "\n",
        "        scores_j = (torch.matmul(Q_r, K_j.transpose(-1, -2)) -\n",
        "                    torch.matmul(Q_i, K_k.transpose(-1, -2)) +\n",
        "                    torch.matmul(Q_j, K_r.transpose(-1, -2)) +\n",
        "                    torch.matmul(Q_k, K_i.transpose(-1, -2)))\n",
        "\n",
        "        scores_k = (torch.matmul(Q_r, K_k.transpose(-1, -2)) +\n",
        "                    torch.matmul(Q_i, K_j.transpose(-1, -2)) -\n",
        "                    torch.matmul(Q_j, K_i.transpose(-1, -2)) +\n",
        "                    torch.matmul(Q_k, K_r.transpose(-1, -2)))\n",
        "\n",
        "        # Scale attention scores\n",
        "        scores_r = scores_r / self.scale_factor\n",
        "        scores_i = scores_i / self.scale_factor\n",
        "        scores_j = scores_j / self.scale_factor\n",
        "        scores_k = scores_k / self.scale_factor\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        weights_r = F.softmax(scores_r, dim=-1)\n",
        "        weights_i = F.softmax(scores_i, dim=-1)\n",
        "        weights_j = F.softmax(scores_j, dim=-1)\n",
        "        weights_k = F.softmax(scores_k, dim=-1)\n",
        "\n",
        "        # Apply dropout if needed\n",
        "        if self.dropout_rate > 0 and self.training:\n",
        "            weights_r = F.dropout(weights_r, p=self.dropout_rate, training=self.training)\n",
        "            weights_i = F.dropout(weights_i, p=self.dropout_rate, training=self.training)\n",
        "            weights_j = F.dropout(weights_j, p=self.dropout_rate, training=self.training)\n",
        "            weights_k = F.dropout(weights_k, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context_r = torch.matmul(weights_r, V_r)\n",
        "        context_i = torch.matmul(weights_i, V_i)\n",
        "        context_j = torch.matmul(weights_j, V_j)\n",
        "        context_k = torch.matmul(weights_k, V_k)\n",
        "\n",
        "        # Concatenate quaternion components\n",
        "        context = torch.cat([context_r, context_i, context_j, context_k], dim=-1)\n",
        "\n",
        "        # Combine heads\n",
        "        context = self.combine_heads(context)\n",
        "\n",
        "        # Apply output projection\n",
        "        output = self.output_transform(context)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "gAdtyQF7jBDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "### To implement full quatformer, replace nn.Linear() with QuaternionTransformation() - will increase compute time"
      ],
      "metadata": {
        "id": "Mc6i6LJzlUsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADD POSITIONAL INFO TO TOKEN EMBEDDINGS\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, dmodel, dropout, padding_idx=None):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Learn position embeddings directly, initialized with small random values\n",
        "        self.pos_encoding = nn.Parameter(torch.zeros(1, max_len, dmodel))\n",
        "        nn.init.normal_(self.pos_encoding, mean=0, std=0.1)\n",
        "\n",
        "        # Set padding positions to zero\n",
        "        if padding_idx is not None:\n",
        "            with torch.no_grad():\n",
        "                self.pos_encoding.data[:, padding_idx, :] = 0.0\n",
        "\n",
        "    def forward(self, embedd):\n",
        "        # Use broadcasting to add positional encodings\n",
        "        embedd = embedd + self.pos_encoding[:, :embedd.size(1), :]\n",
        "        return self.dropout(embedd)"
      ],
      "metadata": {
        "id": "_xsSU4qKrrYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlockQuaternions(nn.Module):\n",
        "    def __init__(self, dmodel, ffnn_hidden_size, num_heads, dropout):\n",
        "        super(TransformerBlockQuaternions, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention with quaternions\n",
        "        self.attention = MultiHeadQuaternionSelfAttention(\n",
        "            input_dim=dmodel,\n",
        "            output_dim=dmodel,\n",
        "            num_heads=num_heads,\n",
        "            dropout_rate=dropout\n",
        "        )\n",
        "\n",
        "        # Use PyTorch's optimized LayerNorm\n",
        "        self.layer_norm1 = nn.LayerNorm(dmodel)\n",
        "\n",
        "        # Full QFFN\n",
        "        # self.ffnn = nn.Sequential(\n",
        "        #     QuaternionTransformation(dmodel, ffnn_hidden_size),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Dropout(dropout),\n",
        "        #     QuaternionTransformation(ffnn_hidden_size, dmodel)\n",
        "        # )\n",
        "\n",
        "        # Partial QFFN\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(dmodel, ffnn_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ffnn_hidden_size, dmodel)\n",
        "        )\n",
        "\n",
        "        self.layer_norm2 = nn.LayerNorm(dmodel)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection and layer normalization\n",
        "        attn_out = self.attention(x)\n",
        "        x = self.layer_norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # Feed-forward network with residual connection and layer normalization\n",
        "        ffnn_out = self.ffnn(x)\n",
        "        x = self.layer_norm2(x + self.dropout(ffnn_out))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "x9AH7q6_rsF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL INTEGRATING BERT EMBEDDINGS\n",
        "class QuaternionsTransformerModel(nn.Module):\n",
        "    def __init__(self, pretrained_model_name='bert-base-uncased', output_size=3,\n",
        "                 n_layers=6, ffnn_hidden_size=None, num_heads=4, dropout=0.1, pooling='max'):\n",
        "        super(QuaternionsTransformerModel, self).__init__()\n",
        "\n",
        "        # BERT model for embeddings - use from_pretrained with cache\n",
        "        self.bert = BertModel.from_pretrained(pretrained_model_name, add_pooling_layer=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pooling = pooling\n",
        "\n",
        "        dmodel = self.bert.config.hidden_size  # Typically 768 for bert-base\n",
        "\n",
        "        if ffnn_hidden_size is None:\n",
        "            ffnn_hidden_size = dmodel * 4\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = LearnablePositionalEncoding(\n",
        "            max_len=512,\n",
        "            dmodel=dmodel,\n",
        "            dropout=dropout,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        # Use ModuleList for efficient instantiation of transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlockQuaternions(\n",
        "                dmodel=dmodel,\n",
        "                ffnn_hidden_size=ffnn_hidden_size,\n",
        "                num_heads=num_heads,\n",
        "                dropout=dropout\n",
        "            )\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(dmodel)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc = nn.Linear(dmodel, output_size)\n",
        "\n",
        "        # Freeze BERT parameters for efficiency (optional)\n",
        "        # for param in self.bert.parameters():\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get BERT embeddings\n",
        "        if not self.bert.training:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        else:\n",
        "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        x = outputs.last_hidden_state  # Shape: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Apply positional encoding\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Pooling - use efficient operations\n",
        "        if self.pooling == 'max':\n",
        "            # More efficient max pooling using adaptive_max_pool1d\n",
        "            x = F.adaptive_max_pool1d(x.transpose(1, 2), 1).squeeze(-1)\n",
        "        else:  # Average Pooling\n",
        "            # Masked average pooling for efficiency\n",
        "            # Create a float mask from the boolean attention mask\n",
        "            mask = attention_mask.unsqueeze(-1).float()  # [batch_size, seq_len, 1]\n",
        "\n",
        "            # Apply mask and calculate sum\n",
        "            masked_x = x * mask\n",
        "\n",
        "            # Sum along sequence dimension\n",
        "            x = masked_x.sum(dim=1)\n",
        "\n",
        "            # Divide by the number of non-padding tokens (sum of mask)\n",
        "            x = x / (mask.sum(dim=1) + 1e-8)  # Add epsilon to avoid division by zero\n",
        "\n",
        "        # Final classification layer\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "e6gsw900dKgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Functions"
      ],
      "metadata": {
        "id": "2O-6ODQUn4p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(input_ids, attention_mask, labels, batch_size):\n",
        "    dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "yE-oAVFEd280"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Eval"
      ],
      "metadata": {
        "id": "y6FrkW52n_5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss\n",
        "\n",
        "def eval_model(model, dataloader, criterion, device):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct_preds += torch.sum(preds == labels)\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_preds.double() / total_preds\n",
        "    return avg_loss, accuracy.item(), all_preds, all_labels"
      ],
      "metadata": {
        "id": "he4ijo86d7gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Workflow"
      ],
      "metadata": {
        "id": "TmSQAsA-oEiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_dataset = load_dataset(\"sjyuxyz/financial-sentiment-analysis\", split=\"train\")\n",
        "    test_dataset = load_dataset(\"sjyuxyz/financial-sentiment-analysis\", split=\"test\")\n",
        "    valid_dataset = load_dataset(\"sjyuxyz/financial-sentiment-analysis\", split=\"valid\")\n",
        "    train_data = pd.DataFrame(train_dataset)\n",
        "    test_data = pd.DataFrame(test_dataset)\n",
        "    valid_data = pd.DataFrame(valid_dataset)\n",
        "\n",
        "    # Initialize the TextPreprocessor\n",
        "    preprocessor = TextPreprocessor(pretrained_model_name='google/bert_uncased_L-4_H-256_A-4', max_len=50)\n",
        "    (train_input_ids, train_attention_mask, train_labels,\n",
        "     val_input_ids, val_attention_mask, val_labels,\n",
        "     test_input_ids, test_attention_mask, test_labels) = preprocessor.preprocess_dataset(\n",
        "        [train_data,test_data,valid_data], text_column='text', label_column='label'\n",
        "    )\n",
        "\n",
        "    # Create DataLoader\n",
        "    batch_size = 64  # Adjust based on your hardware capabilities\n",
        "\n",
        "    train_loader = create_dataloader(train_input_ids, train_attention_mask, train_labels, batch_size)\n",
        "    val_loader = create_dataloader(val_input_ids, val_attention_mask, val_labels, batch_size)\n",
        "    test_loader = create_dataloader(test_input_ids, test_attention_mask, test_labels, batch_size)\n",
        "\n",
        "    # Initialize Model, Loss, Optimizer\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Define model parameters\n",
        "    pretrained_model_name = 'google/bert_uncased_L-4_H-256_A-4'\n",
        "    output_size = 3           # Number of classes: neutral, positive, negative\n",
        "    n_layers = 27             # Number of Transformer blocks; increase for better performance\n",
        "    ffnn_hidden_size = 4096   # If None, defaults to dmodel * 4\n",
        "    num_heads = 8             # Number of attention heads\n",
        "    dropout = 0.1\n",
        "    pooling = 'max'           # 'max' or 'avg'\n",
        "\n",
        "    # Initialize the model\n",
        "    model = QuaternionsTransformerModel(\n",
        "        pretrained_model_name=pretrained_model_name,\n",
        "        output_size=output_size,\n",
        "        n_layers=n_layers,\n",
        "        ffnn_hidden_size=ffnn_hidden_size,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        pooling=pooling\n",
        "    )\n",
        "    model.to(device)\n",
        "    print(count_parameters(model))\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Training Loop: Warning - Full quaternion implementation will increase computation time\n",
        "    num_epochs = 8\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_accuracy, val_preds, val_labels_true = eval_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Save the model if validation accuracy improves\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_quaternions_transformer.pt')\n",
        "            print(\"Model saved.\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Evaluation on Test Set\n",
        "    # Load the best model\n",
        "    model.load_state_dict(torch.load('best_quaternions_transformer.pt'))\n",
        "\n",
        "    test_loss, test_accuracy, test_preds, test_labels_true = eval_model(model, test_loader, criterion, device)\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Detailed Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels_true, test_preds, target_names=['neutral', 'positive', 'negative']))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(test_labels_true, test_preds))\n",
        "    print(count_parameters(model))\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    print(f\"\\nTotal Time Elapsed: {elapsed_time:.2f} seconds ({elapsed_time / 60:.2f} minutes)\")"
      ],
      "metadata": {
        "id": "h51nEan7eCFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FULL QUATFORMER\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dNNpXL9aVyq",
        "outputId": "ba6500db-d95c-4c6e-9802-2a8951f310c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning text data...\n",
            "Tokenizing and encoding text data...\n",
            "Train set: 80029 samples\n",
            "Validation set: 10004 samples\n",
            "Test set: 10004 samples\n",
            "Using device: cuda\n",
            "27190019\n",
            "\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6845\n",
            "Validation Loss: 0.5178 | Validation Accuracy: 0.7937\n",
            "Model saved.\n",
            "\n",
            "Epoch 2/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4807\n",
            "Validation Loss: 0.4380 | Validation Accuracy: 0.8290\n",
            "Model saved.\n",
            "\n",
            "Epoch 3/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3927\n",
            "Validation Loss: 0.3889 | Validation Accuracy: 0.8515\n",
            "Model saved.\n",
            "\n",
            "Epoch 4/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3296\n",
            "Validation Loss: 0.3831 | Validation Accuracy: 0.8578\n",
            "Model saved.\n",
            "\n",
            "Epoch 5/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2841\n",
            "Validation Loss: 0.3686 | Validation Accuracy: 0.8704\n",
            "Model saved.\n",
            "\n",
            "Epoch 6/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2481\n",
            "Validation Loss: 0.3528 | Validation Accuracy: 0.8799\n",
            "Model saved.\n",
            "\n",
            "Epoch 7/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2147\n",
            "Validation Loss: 0.3621 | Validation Accuracy: 0.8815\n",
            "Model saved.\n",
            "\n",
            "Epoch 8/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1898\n",
            "Validation Loss: 0.3744 | Validation Accuracy: 0.8855\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                             "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.3577 | Test Accuracy: 0.8858\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       0.92      0.78      0.85      2094\n",
            "    positive       0.88      0.93      0.91      5038\n",
            "    negative       0.87      0.88      0.87      2872\n",
            "\n",
            "    accuracy                           0.89     10004\n",
            "   macro avg       0.89      0.86      0.88     10004\n",
            "weighted avg       0.89      0.89      0.88     10004\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1642  306  146]\n",
            " [ 104 4702  232]\n",
            " [  42  312 2518]]\n",
            "27190019\n",
            "\n",
            "Total Time Elapsed: 5813.15 seconds (96.89 minutes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PARTIAL QUATFORMER - RERUN WITH SWITCHED OUT COMMENT IN TRANSFORMER BLOCK\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY-aIU3ytR1K",
        "outputId": "b62a60ab-ab20-4e0c-bf20-3bbb42b9d72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning text data...\n",
            "Tokenizing and encoding text data...\n",
            "Train set: 80029 samples\n",
            "Validation set: 10004 samples\n",
            "Test set: 10004 samples\n",
            "Using device: cuda\n",
            "69774851\n",
            "\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.6921\n",
            "Validation Loss: 0.5660 | Validation Accuracy: 0.7827\n",
            "Model saved.\n",
            "\n",
            "Epoch 2/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.4916\n",
            "Validation Loss: 0.4509 | Validation Accuracy: 0.8289\n",
            "Model saved.\n",
            "\n",
            "Epoch 3/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.4062\n",
            "Validation Loss: 0.4251 | Validation Accuracy: 0.8418\n",
            "Model saved.\n",
            "\n",
            "Epoch 4/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.3472\n",
            "Validation Loss: 0.3706 | Validation Accuracy: 0.8617\n",
            "Model saved.\n",
            "\n",
            "Epoch 5/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.3038\n",
            "Validation Loss: 0.3771 | Validation Accuracy: 0.8670\n",
            "Model saved.\n",
            "\n",
            "Epoch 6/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2702\n",
            "Validation Loss: 0.3674 | Validation Accuracy: 0.8715\n",
            "Model saved.\n",
            "\n",
            "Epoch 7/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2394\n",
            "Validation Loss: 0.3439 | Validation Accuracy: 0.8771\n",
            "Model saved.\n",
            "\n",
            "Epoch 8/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2166\n",
            "Validation Loss: 0.3731 | Validation Accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                             "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.3321 | Test Accuracy: 0.8781\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       0.90      0.79      0.84      2094\n",
            "    positive       0.90      0.90      0.90      5038\n",
            "    negative       0.83      0.90      0.86      2872\n",
            "\n",
            "    accuracy                           0.88     10004\n",
            "   macro avg       0.88      0.86      0.87     10004\n",
            "weighted avg       0.88      0.88      0.88     10004\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1644  268  182]\n",
            " [ 137 4544  357]\n",
            " [  51  224 2597]]\n",
            "69774851\n",
            "\n",
            "Total Time Elapsed: 4740.63 seconds (79.01 minutes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smeQ5AEntVx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Visuals"
      ],
      "metadata": {
        "id": "CyZocONHevb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# # FULL QUATFORMER\n",
        "# conf_matrix = np.array([\n",
        "#     [1642, 306, 146],\n",
        "#     [104, 4702, 232],\n",
        "#     [42, 312, 2518]\n",
        "# ])\n",
        "\n",
        "# # PARTIAL QUATFORMER\n",
        "# conf_matrix = np.array([\n",
        "#     [1644, 268, 182],\n",
        "#     [137, 4544, 357],\n",
        "#     [51, 224, 2597]\n",
        "# ])\n",
        "\n",
        "# PRETRAINED BERT - FROM OTHER NOTEBOOK\n",
        "conf_matrix = np.array([\n",
        "    [1819, 198, 77],\n",
        "    [123, 4749, 166],\n",
        "    [67, 221, 2584]\n",
        "])\n",
        "\n",
        "# Define class labels\n",
        "class_labels = ['Neutral', 'Positive', 'Negative']\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
        "\n",
        "# Create a figure\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Normalize the confusion matrix by row (true label)\n",
        "row_sums = conf_matrix.sum(axis=1)\n",
        "norm_conf_matrix = conf_matrix / row_sums[:, np.newaxis]\n",
        "\n",
        "# Create the confusion matrix display\n",
        "display = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=norm_conf_matrix,\n",
        "    display_labels=class_labels\n",
        ")\n",
        "\n",
        "# Plot with enhanced styling\n",
        "display.plot(\n",
        "    cmap='Blues',           # Blue color map\n",
        "    values_format='.1%',    # Show as percentages with 1 decimal place\n",
        "    colorbar=True,          # Show color scale\n",
        "    xticks_rotation=45,     # Rotate x labels for better readability\n",
        "    ax=plt.gca()            # Get current axis\n",
        ")\n",
        "\n",
        "# Add title and adjust labels\n",
        "plt.title(f'Pretrained BERT: {accuracy:.2%}', fontsize=16, pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=14, labelpad=10)\n",
        "plt.ylabel('True Label', fontsize=14, labelpad=10)\n",
        "\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "GvxAXL7OegqS",
        "outputId": "e3b166a4-78c8-4d85-fcd1-740711c219b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lj_rY3WjmYFc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}